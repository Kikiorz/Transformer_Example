{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686ce9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrp/miniconda3/envs/llm_scrach/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import einops\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70e1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载器定义完成\n"
     ]
    }
   ],
   "source": [
    "# 数据生成部分，定义词典（数据生成的范围）\n",
    "# 定义字典\n",
    "zidian_x = '<SOS>,<EOS>,<PAD>,0,1,2,3,4,5,6,7,8,9,q,w,e,r,t,y,u,i,o,p,a,s,d,f,g,h,j,k,l,z,x,c,v,b,n,m'\n",
    "zidian_x = {word: i for i, word in enumerate(zidian_x.split(','))}\n",
    "\n",
    "zidian_xr = [k for k, v in zidian_x.items()]\n",
    "\n",
    "zidian_y = {k.upper(): v for k, v in zidian_x.items()}\n",
    "\n",
    "zidian_yr = [k for k, v in zidian_y.items()]\n",
    "# 生成数据逻辑就是：X是正序小写数字，Y是逆序大写互补数字\n",
    "def get_data():\n",
    "    # 定义词集合\n",
    "    words = [\n",
    "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'q', 'w', 'e', 'r',\n",
    "        't', 'y', 'u', 'i', 'o', 'p', 'a', 's', 'd', 'f', 'g', 'h', 'j', 'k',\n",
    "        'l', 'z', 'x', 'c', 'v', 'b', 'n', 'm'\n",
    "    ]\n",
    "\n",
    "    # 定义每个词被选中的概率\n",
    "    p = np.array([\n",
    "        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
    "        13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26\n",
    "    ])\n",
    "    p = p / p.sum()\n",
    "\n",
    "    # 随机选n个词\n",
    "    n = random.randint(30, 48)\n",
    "    x = np.random.choice(words, size=n, replace=True, p=p)\n",
    "\n",
    "    # 采样的结果就是x\n",
    "    x = x.tolist()\n",
    "\n",
    "    # y是对x的变换得到的\n",
    "    # 字母大写,数字取10以内的互补数\n",
    "    def f(i):\n",
    "        i = i.upper()\n",
    "        if not i.isdigit():\n",
    "            return i\n",
    "        i = 9 - int(i)\n",
    "        return str(i)\n",
    "\n",
    "    y = [f(i) for i in x]\n",
    "    y = y + [y[-1]]\n",
    "    # 逆序\n",
    "    y = y[::-1]\n",
    "\n",
    "    # 加上首尾符号\n",
    "    x = ['<SOS>'] + x + ['<EOS>']\n",
    "    y = ['<SOS>'] + y + ['<EOS>']\n",
    "\n",
    "    # 补pad到固定长度\n",
    "    x = x + ['<PAD>'] * 50\n",
    "    y = y + ['<PAD>'] * 51\n",
    "    x = x[:50]\n",
    "    y = y[:51]\n",
    "\n",
    "    # 编码成数据\n",
    "    x = [zidian_x[i] for i in x]\n",
    "    y = [zidian_y[i] for i in y]\n",
    "\n",
    "    # 转tensor\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# 定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1000000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return get_data()\n",
    "\n",
    "# 数据加载器\n",
    "\n",
    "print('数据加载器定义完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f23c75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载器定义完成\n"
     ]
    }
   ],
   "source": [
    "# mask\n",
    "import torch\n",
    "\n",
    "from data import zidian_x, zidian_y\n",
    "\n",
    "# mask pad\n",
    "def mask_pad(data):\n",
    "    mask = data == zidian_x['<PAD>']\n",
    "\n",
    "    # [b, 50] -> [b, 1, 1, 50]\n",
    "    mask = mask.reshape(-1, 1, 1, 50)\n",
    "\n",
    "    # 在计算注意力时,是计算50个词和50个词相互之间的注意力,所以是个50*50的矩阵\n",
    "    # 是pad的列是true,意味着任何词对pad的注意力都是0\n",
    "    # 但是pad本身对其他词的注意力并不是0\n",
    "    # 所以是pad的行不是true\n",
    "\n",
    "    # 复制n次\n",
    "    # [b, 1, 1, 50] -> [b, 1, 50, 50]\n",
    "    mask = mask.expand(-1, 1, 50, 50)\n",
    "\n",
    "    return mask\n",
    "\n",
    "# 在Y预测阶段变为上三角\n",
    "def mask_tril(data):\n",
    "    # b句话,每句话50个词,这里是还没embed的\n",
    "    # data = [b, 50]\n",
    "\n",
    "    # 50*50的矩阵表示每个词对其他词是否可见\n",
    "    # 上三角矩阵,不包括对角线,意味着,对每个词而言,他只能看到他自己,和他之前的词,而看不到之后的词\n",
    "    # [1, 50, 50]\n",
    "    \"\"\"\n",
    "    [[0, 1, 1, 1, 1],\n",
    "     [0, 0, 1, 1, 1],\n",
    "     [0, 0, 0, 1, 1],\n",
    "     [0, 0, 0, 0, 1],\n",
    "     [0, 0, 0, 0, 0]]\"\"\"\n",
    "    tril = 1 - torch.tril(torch.ones(1, 50, 50, dtype=torch.long)).to(device)\n",
    "\n",
    "    # 判断y当中每个词是不是pad,如果是pad则不可见\n",
    "    # [b, 50]\n",
    "    mask = data == zidian_y['<PAD>']\n",
    "\n",
    "    # 变形+转型,为了之后的计算\n",
    "    # [b, 1, 50]\n",
    "    mask = mask.unsqueeze(1).long()\n",
    "\n",
    "    # mask和tril求并集\n",
    "    # [b, 1, 50] + [1, 50, 50] -> [b, 50, 50]\n",
    "    mask = mask + tril\n",
    "\n",
    "    # 转布尔型\n",
    "    mask = mask > 0\n",
    "\n",
    "    # 转布尔型,增加一个维度,便于后续的计算\n",
    "    mask = (mask == 1).unsqueeze(dim=1)\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577b5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myPositionEmbedding, 位置编码\n",
    "class myPositionEmbedding(torch.nn.Module):\n",
    "    def __init__(self,voc_size,max_len,emb_dim):\n",
    "        super().__init__()\n",
    "        # 创建嵌入层 \"b,50->b,50,32\"\n",
    "        self.emb=torch.nn.Embedding(voc_size,emb_dim)\n",
    "        self.emb.weight.data.normal_(0, 0.1)\n",
    "\n",
    "        # 创建PE参数记得把他移动到模型中哦\n",
    "        pe=torch.zeros(max_len, emb_dim,dtype=torch.float32)\n",
    "        # for嵌套复杂度太高了，\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        i = torch.arange(0, emb_dim, 2, dtype=torch.float32)\n",
    "        inv_freq = 1.0 / (1.0e-3 ** (i / emb_dim))\n",
    "        # 广播机制（50,1） （32）\n",
    "        argument = pos * inv_freq\n",
    "        pe[:, 0::2] = torch.sin(argument)\n",
    "        pe[:, 1::2] = torch.cos(argument)\n",
    "        # 送到模型中，并且不更新\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.emb(x)\n",
    "        return x+self.pe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d4564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头注意力机制\n",
    "class MultiHead(torch.nn.Module):\n",
    "    def __init__(self,head_num,in_ch):\n",
    "        super().__init__()\n",
    "        self.wq=torch.nn.Linear(in_ch,in_ch)\n",
    "        self.wk=torch.nn.Linear(in_ch,in_ch)\n",
    "        self.wv=torch.nn.Linear(in_ch,in_ch)\n",
    "        self.num_head=head_num\n",
    "        self.in_ch=in_ch\n",
    "        self.norm = torch.nn.LayerNorm(normalized_shape=in_ch, elementwise_affine=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.out_fc=torch.nn.Linear(in_ch,in_ch)\n",
    "\n",
    "    def forward(self,x,mask,cros_flag,VX=0):\n",
    "        # x->[b,l,d]\n",
    "        if cros_flag==0:\n",
    "            res=x.clone()\n",
    "            x = self.norm(x)\n",
    "            Q= einops.rearrange(self.wq(x),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "            K=einops.rearrange(self.wk(x),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "            V=einops.rearrange(self.wv(x),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "            \n",
    "            # score = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "            # score /= 8 ** 0.5\n",
    "            # score = score.masked_fill_(mask, -float('inf'))\n",
    "            # score = torch.softmax(score, dim=-1)\n",
    "            # z= torch.matmul(score, V)\n",
    "            z=torch.softmax(((Q@K.permute(0, 1, 3, 2))/((self.in_ch/self.num_head)**0.5)).masked_fill_(mask, -float('inf')),dim=-1)@V\n",
    "\n",
    "            z=einops.rearrange(z,\"... a b c -> ... b (a c)\")\n",
    "\n",
    "            return self.dropout(self.out_fc(z))+res\n",
    "        else:\n",
    "            res=x.clone()\n",
    "            x = self.norm(x)\n",
    "            VX= self.norm(VX)\n",
    "            Q= einops.rearrange(self.wq(x),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "            K=einops.rearrange(self.wk(VX),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "            V=einops.rearrange(self.wv(VX),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "\n",
    "            z=torch.softmax(((Q@K.permute(0, 1, 3, 2))/((self.in_ch/self.num_head)**0.5)).masked_fill_(mask, -float('inf')),dim=-1)@V\n",
    "            # score = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "            # score /= 8 ** 0.5\n",
    "            # score = score.masked_fill_(mask, -float('inf'))\n",
    "            # score = torch.softmax(score, dim=-1)\n",
    "            # z= torch.matmul(score, V)\n",
    "            z=einops.rearrange(z,\"... a b c -> ... b (a c)\")\n",
    "\n",
    "            return self.dropout(self.out_fc(z))+res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ca4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全连接组成\n",
    "class FullyConnected(torch.nn.Module):\n",
    "    def __init__(self,in_ch):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=in_ch, out_features=in_ch*2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=in_ch*2, out_features=in_ch),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "        )\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(normalized_shape=32,\n",
    "        elementwise_affine=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        res=x.clone()\n",
    "        x=self.norm(x)\n",
    "        x=self.fc(x)\n",
    "        return x+res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d92aeb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器组成\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self,num_head,in_ch ):\n",
    "        super().__init__()\n",
    "        self.mul=MultiHead(num_head,in_ch=in_ch)\n",
    "        self.fc=FullyConnected(in_ch)\n",
    "    def forward(self,x,mask):\n",
    "        # x [b,long,emb]\n",
    "        # 多头注意力\n",
    "        x=self.mul(x,mask,cros_flag=0)\n",
    "        # 前馈神经网络\n",
    "        # x [b,long,emb]\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,num_head,in_ch):\n",
    "        super().__init__()\n",
    "        self.layer_1 = EncoderLayer(num_head,in_ch)\n",
    "        self.layer_2 = EncoderLayer(num_head,in_ch)\n",
    "        self.layer_3 = EncoderLayer(num_head,in_ch)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.layer_1(x, mask)\n",
    "        x = self.layer_2(x, mask)\n",
    "        x = self.layer_3(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e8d5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器层\n",
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self,num_head,in_ch):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mul1 = MultiHead(num_head,in_ch)\n",
    "        self.mul2 = MultiHead(num_head,in_ch)\n",
    "\n",
    "        self.fc = FullyConnected(in_ch)\n",
    "\n",
    "    def forward(self, x, y, mask_pad_x, mask_tril_y):\n",
    "        # 先计算y的自注意力,维度不变\n",
    "        y = self.mul1(y, mask_tril_y, cros_flag=0)\n",
    "        # 结合x和y的注意力计算,维度不变\n",
    "        # [b, 50, 32],[b, 50, 32] -> [b, 50, 32]\n",
    "        y = self.mul2(y, mask_pad_x, cros_flag=1,VX=x)\n",
    "        y = self.fc(y)\n",
    "        return y\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self,num_head,in_ch):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_1 = DecoderLayer(num_head,in_ch)\n",
    "        self.layer_2 = DecoderLayer(num_head,in_ch)\n",
    "        self.layer_3 = DecoderLayer(num_head,in_ch)\n",
    "\n",
    "    def forward(self, x, y, mask_pad_x, mask_tril_y):\n",
    "        y = self.layer_1(x, y, mask_pad_x, mask_tril_y)\n",
    "        y = self.layer_2(x, y, mask_pad_x, mask_tril_y)\n",
    "        y = self.layer_3(x, y, mask_pad_x, mask_tril_y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28396e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self,voc_size,max_len,num_head,in_ch):\n",
    "        super().__init__()\n",
    "        # self.embed_x = PositionEmbedding()\n",
    "        # self.embed_y = PositionEmbedding()\n",
    "        self.embed_x = myPositionEmbedding(voc_size,max_len,in_ch)\n",
    "        self.embed_y = myPositionEmbedding(voc_size,max_len,in_ch)\n",
    "        self.encoder = Encoder(num_head,in_ch)\n",
    "        self.decoder = Decoder(num_head,in_ch)\n",
    "        self.fc_out = torch.nn.Linear(in_ch, 39)\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        mask_x=mask_pad(x)\n",
    "        mask_y=mask_tril(y)\n",
    "        x=self.embed_x(x)\n",
    "        y=self.embed_y(y)\n",
    "        # x[b,50]->[b,50,32]\n",
    "        x=self.encoder(x,mask_x)\n",
    "        y=self.decoder(x,y,mask_x,mask_y)\n",
    "        y=self.fc_out(y)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592c13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset=Dataset(),\n",
    "                                     batch_size=128,\n",
    "                                     drop_last=True,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=None)\n",
    "model = Transformer(39,50,4,32)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "loss_func.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "sched = torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9bb327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.002 3.80072283744812 0.027413127413127413\n",
      "0 200 0.002 3.208475351333618 0.11238489005825973\n",
      "0 400 0.002 2.970705270767212 0.1779530588687957\n",
      "0 600 0.002 1.875807762145996 0.4497254307896232\n",
      "0 800 0.002 1.0439825057983398 0.6754167465031615\n",
      "0 1000 0.002 0.7463297843933105 0.7639775585219578\n",
      "0 1200 0.002 0.580879807472229 0.8214015151515152\n",
      "0 1400 0.002 0.42304712533950806 0.8677717810331534\n",
      "0 1600 0.002 0.33712196350097656 0.9029695479477965\n",
      "0 1800 0.002 0.28465762734413147 0.9187358916478555\n",
      "0 2000 0.002 0.1992313116788864 0.9419149334832303\n",
      "0 2200 0.002 0.21679513156414032 0.9339215318190351\n",
      "0 2400 0.002 0.14478090405464172 0.9580527227246488\n",
      "0 2600 0.002 0.11044387519359589 0.9698588325066768\n",
      "0 2800 0.002 0.1027359664440155 0.9681176021382207\n",
      "0 3000 0.002 0.08668502420186996 0.9746954076850984\n",
      "0 3200 0.002 0.0877983421087265 0.9735632183908046\n",
      "0 3400 0.002 0.08615375310182571 0.9775708040296521\n",
      "0 3600 0.002 0.08491507917642593 0.9769200930954228\n",
      "0 3800 0.002 0.05847073346376419 0.9827160493827161\n",
      "0 4000 0.002 0.05341241881251335 0.9850689850689851\n",
      "0 4200 0.002 0.04367455840110779 0.9858129550121336\n",
      "0 4400 0.002 0.04372986778616905 0.9880114176974311\n",
      "0 4600 0.002 0.04171081259846687 0.9871333964049196\n",
      "0 4800 0.002 0.043343864381313324 0.9876940552820901\n",
      "0 5000 0.002 0.04711773991584778 0.9852104664391353\n",
      "0 5200 0.002 0.05540386214852333 0.9867636092468307\n",
      "0 5400 0.002 0.04231340438127518 0.9876780901039661\n",
      "0 5600 0.002 0.036760274320840836 0.9884704073789393\n",
      "0 5800 0.002 0.030722342431545258 0.9906120916259857\n",
      "0 6000 0.002 0.028152216225862503 0.9930228172732416\n",
      "0 6200 0.002 0.021503819152712822 0.9935471255377395\n",
      "0 6400 0.002 0.030047887936234474 0.9930293896006028\n",
      "0 6600 0.002 0.025699123740196228 0.9923019151333083\n",
      "0 6800 0.002 0.019576245918869972 0.9956306990881459\n",
      "0 7000 0.002 0.02099093422293663 0.9940269749518305\n",
      "0 7200 0.002 0.01326005719602108 0.9959677419354839\n",
      "0 7400 0.002 0.023646783083677292 0.9940108553247239\n",
      "0 7600 0.002 0.021636098623275757 0.9934924078091106\n",
      "0 7800 0.002 0.020257635042071342 0.9948669201520912\n",
      "1 0 0.002 0.017921872437000275 0.9960600375234522\n",
      "1 200 0.002 0.01317314337939024 0.9968992248062015\n",
      "1 400 0.002 0.011524410918354988 0.996926623127161\n",
      "1 600 0.002 0.007730841636657715 0.997907949790795\n",
      "1 800 0.002 0.017365235835313797 0.9967692892436336\n",
      "1 1000 0.002 0.010646427050232887 0.9977460555972952\n",
      "1 1200 0.002 0.014665262773633003 0.9964512514008218\n",
      "1 1400 0.002 0.014062618836760521 0.9974956655750337\n",
      "1 1600 0.002 0.019633088260889053 0.9948522402287894\n",
      "1 1800 0.002 0.015010148286819458 0.9961111111111111\n",
      "1 2000 0.002 0.00493467366322875 0.9986832204665161\n",
      "1 2200 0.002 0.10597621649503708 0.9814282979130767\n",
      "1 2400 0.002 0.012056783773005009 0.9967301404116177\n",
      "1 2600 0.002 0.005867392290383577 0.9984770607272035\n",
      "1 2800 0.002 0.00840586144477129 0.9973604826546003\n",
      "1 3000 0.002 0.013476355001330376 0.9965068891907627\n",
      "1 3200 0.002 0.011419362388551235 0.9963049397121743\n",
      "1 3400 0.002 0.004464819096028805 0.9986468200270636\n",
      "1 3600 0.002 0.009249941445887089 0.9975563909774436\n",
      "1 3800 0.002 0.006462549325078726 0.9990624414025877\n",
      "1 4000 0.002 0.010867387987673283 0.9967772511848341\n",
      "1 4200 0.002 0.0035822195932269096 0.9988601823708206\n",
      "1 4400 0.002 0.003235272131860256 0.9990541051835036\n",
      "1 4600 0.002 0.003371269442141056 0.9986595174262735\n",
      "1 4800 0.002 0.00341811403632164 0.9988290398126464\n",
      "1 5000 0.002 0.005119506735354662 0.9984329089128305\n",
      "1 5200 0.002 0.006082634907215834 0.998272884283247\n",
      "1 5400 0.002 0.007488864939659834 0.9981146304675717\n",
      "1 5600 0.002 0.005887932144105434 0.9980861244019139\n",
      "1 5800 0.002 0.002385662868618965 0.9994367254975591\n",
      "1 6000 0.002 0.006537200883030891 0.9979162720212161\n",
      "1 6200 0.002 0.005018001422286034 0.9988730277986476\n",
      "1 6400 0.002 0.0023290582466870546 0.9992395437262357\n",
      "1 6600 0.002 0.0024861558340489864 0.9996217851739788\n",
      "1 6800 0.002 0.002402968006208539 0.9994300911854104\n",
      "1 7000 0.002 0.002926261629909277 0.9994305239179955\n",
      "1 7200 0.002 0.0036038467660546303 0.999247412982126\n",
      "1 7400 0.002 0.003273527603596449 0.9992459943449575\n",
      "1 7600 0.002 0.0012144548818469048 0.9998088685015291\n",
      "1 7800 0.002 0.001085688010789454 0.999629148896718\n",
      "2 0 0.002 0.001721662119962275 0.9992408426646422\n",
      "2 200 0.002 0.0027921136934310198 0.9992492492492493\n",
      "2 400 0.002 0.006194210145622492 0.998287996956439\n",
      "2 600 0.002 0.002206604927778244 0.9994183792167507\n",
      "2 800 0.002 0.002983405953273177 0.9990393852065321\n",
      "2 1000 0.002 0.04000302404165268 0.9918188736681888\n",
      "2 1200 0.002 0.001511231530457735 0.9996136758740584\n",
      "2 1400 0.002 0.0014545010635629296 0.9996287358455541\n",
      "2 1600 0.002 0.0006108593079261482 1.0\n",
      "2 1800 0.002 0.0014792656293138862 0.9994342824816141\n",
      "2 2000 0.002 0.0026185070164501667 0.999429982899487\n",
      "2 2200 0.002 0.0004659160040318966 1.0\n",
      "2 2400 0.002 0.0011933066416531801 0.9996193376475067\n",
      "2 2600 0.002 0.001097650034353137 0.9998106777735706\n",
      "2 2800 0.002 0.009222610853612423 0.9978975535168195\n",
      "2 3000 0.002 0.0011259595630690455 0.999810641923878\n",
      "2 3200 0.002 0.0022283524740487337 0.9988483685220729\n",
      "2 3400 0.002 0.0005108020268380642 1.0\n",
      "2 3600 0.002 0.0021373641211539507 0.9992438563327032\n",
      "2 3800 0.002 0.0013558784266933799 0.9994236311239193\n",
      "2 4000 0.002 0.002263701520860195 0.9994297662041437\n",
      "2 4200 0.002 0.0012725169071927667 0.9996216420734014\n",
      "2 4400 0.002 0.004988932982087135 0.9990417784591797\n",
      "2 4600 0.002 0.0029400296043604612 0.9990824004404478\n",
      "2 4800 0.002 0.0020416348706930876 0.9990514133940429\n",
      "2 5000 0.002 0.001464484608732164 0.999805900621118\n",
      "2 5200 0.002 0.001162632368505001 0.9996112730806609\n",
      "2 5400 0.002 0.0013357294956222177 0.9996209249431387\n",
      "2 5600 0.002 0.006251429207623005 0.9981280419318608\n",
      "2 5800 0.002 0.008088202215731144 0.99867197875166\n",
      "2 6000 0.002 0.0019532309379428625 0.9996228549877428\n",
      "2 6200 0.002 0.0018894883105531335 0.9996272828922848\n",
      "2 6400 0.002 0.002090692752972245 0.999415318651335\n",
      "2 6600 0.002 0.002019481733441353 0.9994385176866929\n",
      "2 6800 0.002 0.0005221019964665174 1.0\n",
      "2 7000 0.002 0.0004998786607757211 1.0\n",
      "2 7200 0.002 0.0005959180416539311 0.9998047637641546\n",
      "2 7400 0.002 0.00029204224119894207 1.0\n",
      "2 7600 0.002 0.0011267566587775946 0.9996127783155857\n",
      "2 7800 0.002 0.0005679439636878669 0.9998050302203159\n",
      "3 0 0.001 0.0014644990442320704 0.9996172981247609\n",
      "3 200 0.001 0.0003842426522169262 1.0\n",
      "3 400 0.001 0.0003837093827314675 0.9998060888113244\n",
      "3 600 0.001 0.0001448549301130697 1.0\n",
      "3 800 0.001 0.00032684183679521084 1.0\n",
      "3 1000 0.001 0.00029731367249041796 1.0\n",
      "3 1200 0.001 0.0005031673354096711 0.9998113563478589\n",
      "3 1400 0.001 0.0004772399552166462 0.9998078401229823\n",
      "3 1600 0.001 0.000359264959115535 0.9998115696250236\n",
      "3 1800 0.001 0.0008359456551261246 0.999809994299829\n",
      "3 2000 0.001 0.0003468901268206537 0.999813223757938\n",
      "3 2200 0.001 0.0007504324312321842 0.9996191926884996\n",
      "3 2400 0.001 0.0008817509515210986 0.9998126288176878\n",
      "3 2600 0.001 0.00016681991110090166 1.0\n",
      "3 2800 0.001 0.0009991307742893696 0.9998095238095238\n",
      "3 3000 0.001 0.00044611369958147407 0.9998072103335262\n",
      "3 3200 0.001 0.00037958959001116455 1.0\n",
      "3 3400 0.001 0.0006032075034454465 0.9998072103335262\n",
      "3 3600 0.001 0.0006010126089677215 0.9996212838477561\n",
      "3 3800 0.001 0.00010484361700946465 1.0\n",
      "3 4000 0.001 0.001286970917135477 0.9996135265700483\n",
      "3 4200 0.001 0.00032503061811439693 1.0\n",
      "3 4400 0.001 0.0010887044481933117 0.9996127783155857\n",
      "3 4600 0.001 0.0003149828116875142 0.9998114985862394\n",
      "3 4800 0.001 0.0015924215549603105 0.9996174445294568\n",
      "3 5000 0.001 0.00044877410982735455 1.0\n",
      "3 5200 0.001 0.0004379063902888447 0.9998119947358526\n",
      "3 5400 0.001 0.0007059876224957407 0.9996146435452794\n",
      "3 5600 0.001 0.0017596209654584527 0.9994297662041437\n",
      "3 5800 0.001 0.0026943376287817955 0.9994245156339919\n",
      "3 6000 0.001 0.0007328708888962865 0.9996153106366609\n",
      "3 6200 0.001 0.002781945513561368 0.9994241842610365\n",
      "3 6400 0.001 0.0010303636081516743 0.9998104265402844\n",
      "3 6600 0.001 0.0004787447687704116 0.9998114274938714\n",
      "3 6800 0.001 0.00138279318343848 0.9994283536585366\n",
      "3 7000 0.001 0.002596816513687372 0.9992316557817903\n",
      "3 7200 0.001 0.00019735498062800616 1.0\n",
      "3 7400 0.001 0.0004304420144762844 0.9998083189572551\n",
      "3 7600 0.001 0.001114684622734785 0.9994322482967449\n",
      "3 7800 0.001 0.0002006307477131486 1.0\n",
      "4 0 0.001 0.0001994821213884279 1.0\n",
      "4 200 0.001 0.005098084919154644 0.9994186046511628\n",
      "4 400 0.001 0.000682590645737946 0.9996189024390244\n",
      "4 600 0.001 0.00026015081675723195 1.0\n",
      "4 800 0.001 0.000223454087972641 1.0\n",
      "4 1000 0.001 0.0005479772225953639 0.999804801873902\n",
      "4 1200 0.001 0.0009529292583465576 0.9998102826788086\n",
      "4 1400 0.001 0.0005532040959224105 1.0\n",
      "4 1600 0.001 0.0007858986500650644 0.9998086490623804\n",
      "4 1800 0.001 0.00030946251354180276 0.9998149518874907\n",
      "4 2000 0.001 0.00029219925636425614 1.0\n",
      "4 2200 0.001 0.0021594136487692595 0.99961962723469\n",
      "4 2400 0.001 0.0029016099870204926 0.9994332136784432\n",
      "4 2600 0.001 0.00010883564391406253 1.0\n",
      "4 2800 0.001 0.0007498120539821684 0.9998102826788086\n",
      "4 3000 0.001 0.000738087750505656 0.9998078770413065\n",
      "4 3200 0.001 0.0002829129807651043 1.0\n",
      "4 3400 0.001 0.0005619904841296375 0.9998073588903872\n",
      "4 3600 0.001 0.0010732582304626703 0.9994279176201373\n",
      "4 3800 0.001 0.00040590716525912285 0.9998095238095238\n",
      "4 4000 0.001 0.001742859836667776 0.9998087222647284\n",
      "4 4200 0.001 0.0005644449847750366 0.9998100664767331\n",
      "4 4400 0.001 0.0003104271017946303 1.0\n",
      "4 4600 0.001 0.00020718923769891262 1.0\n",
      "4 4800 0.001 0.0019475846784189343 0.9996181019667749\n",
      "4 5000 0.001 0.0009975219145417213 0.9998160412067697\n",
      "4 5200 0.001 0.0005414311308413744 0.9994215194755109\n",
      "4 5400 0.001 9.776939259609208e-05 1.0\n",
      "4 5600 0.001 0.0007404956850223243 0.9996225702962823\n",
      "4 5800 0.001 0.0005022970726713538 0.9998120654012403\n",
      "4 6000 0.001 0.00021903593733441085 1.0\n",
      "4 6200 0.001 0.00037115527084097266 0.9998100664767331\n",
      "4 6400 0.001 0.00047226290917024016 0.999810606060606\n",
      "4 6600 0.001 0.0002055845397990197 1.0\n",
      "4 6800 0.001 0.0018134989077225327 0.9988659988659989\n",
      "4 7000 0.001 7.648145401617512e-05 1.0\n",
      "4 7200 0.001 0.00015327284927479923 1.0\n",
      "4 7400 0.001 0.00041188730392605066 0.9998087588449034\n",
      "4 7600 0.001 0.00032625827589072287 0.9998084658111472\n",
      "4 7800 0.001 0.00023829111887607723 1.0\n",
      "5 0 0.001 0.0003677808854263276 0.9998063516653757\n",
      "5 200 0.001 0.00014818082854617387 1.0\n",
      "5 400 0.001 4.098094723303802e-05 1.0\n",
      "5 600 0.001 0.00032397659379057586 1.0\n",
      "5 800 0.001 0.0004194126231595874 0.9998103905953736\n",
      "5 1000 0.001 0.00015076920681167394 1.0\n",
      "5 1200 0.001 7.377584552159533e-05 1.0\n",
      "5 1400 0.001 7.095384353306144e-05 1.0\n",
      "5 1600 0.001 0.0054103778675198555 0.999423076923077\n",
      "5 1800 0.001 9.594165021553636e-05 1.0\n",
      "5 2000 0.001 0.0011393745662644506 0.9996220710506425\n",
      "5 2200 0.001 0.0002249095996376127 1.0\n",
      "5 2400 0.001 0.0006092822295613587 0.9998119947358526\n",
      "5 2600 0.001 0.0006892047822475433 0.9998136067101584\n",
      "5 2800 0.001 9.656428301241249e-05 1.0\n",
      "5 3000 0.001 0.0017139859264716506 0.9996209249431387\n",
      "5 3200 0.001 0.00032033005845732987 0.9998072103335262\n",
      "5 3400 0.001 0.00015490716032218188 1.0\n",
      "5 3600 0.001 0.000595527293626219 0.99962469506474\n",
      "5 3800 0.001 0.0012479240540415049 0.9996181019667749\n",
      "5 4000 0.001 0.00012257568596396595 1.0\n",
      "5 4200 0.001 0.00036600642488338053 1.0\n",
      "5 4400 0.001 4.589710079017095e-05 1.0\n",
      "5 4600 0.001 0.00035178428515791893 0.9998036520714706\n",
      "5 4800 0.001 5.9635753132170066e-05 1.0\n",
      "5 5000 0.001 0.0002940417325589806 0.9998077662437524\n",
      "5 5200 0.001 0.00011782886576838791 1.0\n",
      "5 5400 0.001 0.0003932659747079015 0.9998075072184793\n",
      "5 5600 0.001 0.00012874747335445136 1.0\n",
      "5 5800 0.001 0.0005525389569811523 0.9998140226892319\n",
      "5 6000 0.001 0.0005615467089228332 0.9998093785741518\n",
      "5 6200 0.001 0.0004092479357495904 0.9998080982536941\n",
      "5 6400 0.001 0.000180709597771056 1.0\n",
      "5 6600 0.001 0.0015308044385164976 0.9998142299832807\n",
      "5 6800 0.001 0.0001666938333073631 1.0\n",
      "5 7000 0.001 0.0006613541627302766 0.9998038061604866\n",
      "5 7200 0.001 4.412298949318938e-05 1.0\n",
      "5 7400 0.001 9.836634126259014e-05 1.0\n",
      "5 7600 0.001 0.0002123677550116554 1.0\n",
      "5 7800 0.001 0.0002260016044601798 1.0\n",
      "6 0 0.0005 0.00047776548308320343 0.9998090874379534\n",
      "6 200 0.0005 3.320282849017531e-05 1.0\n",
      "6 400 0.0005 0.00010746949556050822 1.0\n",
      "6 600 0.0005 9.846283501246944e-05 1.0\n",
      "6 800 0.0005 5.838128345203586e-05 1.0\n",
      "6 1000 0.0005 7.551791350124404e-05 1.0\n",
      "6 1200 0.0005 0.0002675861760508269 1.0\n",
      "6 1400 0.0005 1.153506582340924e-05 1.0\n",
      "6 1600 0.0005 2.1491325242095627e-05 1.0\n",
      "6 1800 0.0005 1.8720988009590656e-05 1.0\n",
      "6 2000 0.0005 0.00010934699093922973 1.0\n",
      "6 2200 0.0005 5.119194247527048e-05 1.0\n",
      "6 2400 0.0005 3.9211470721056685e-05 1.0\n",
      "6 2600 0.0005 6.481313175754622e-05 1.0\n",
      "6 2800 0.0005 0.00014606148761231452 1.0\n",
      "6 3000 0.0005 3.4826061892090365e-05 1.0\n",
      "6 3200 0.0005 0.00011478664237074554 1.0\n",
      "6 3400 0.0005 3.9767208363628015e-05 1.0\n",
      "6 3600 0.0005 0.00010427244706079364 1.0\n",
      "6 3800 0.0005 0.00010208848107140511 1.0\n",
      "6 4000 0.0005 5.310020787874237e-05 1.0\n",
      "6 4200 0.0005 0.00018916984845418483 0.9998105342932929\n",
      "6 4400 0.0005 0.0002908048627432436 0.9998163789937569\n",
      "6 4600 0.0005 1.6558009519940242e-05 1.0\n",
      "6 4800 0.0005 0.00021537876455113292 1.0\n",
      "6 5000 0.0005 3.61015772796236e-05 1.0\n",
      "6 5200 0.0005 1.6983880414045416e-05 1.0\n",
      "6 5400 0.0005 0.00011582935985643417 1.0\n",
      "6 5600 0.0005 1.6722953660064377e-05 1.0\n",
      "6 5800 0.0005 7.495341560570523e-05 1.0\n",
      "6 6000 0.0005 0.0014345175586640835 0.9998125234345707\n",
      "6 6200 0.0005 0.00017666866187937558 1.0\n",
      "6 6400 0.0005 0.001062316121533513 0.9998072474942175\n",
      "6 6600 0.0005 4.9043865146813914e-05 1.0\n",
      "6 6800 0.0005 6.0362817748682573e-05 1.0\n",
      "6 7000 0.0005 5.365718971006572e-05 1.0\n",
      "6 7200 0.0005 6.238478817977011e-05 1.0\n",
      "6 7400 0.0005 0.0011823360109701753 0.9996155324875048\n",
      "6 7600 0.0005 2.9290666134329513e-05 1.0\n",
      "6 7800 0.0005 4.430050466908142e-05 1.0\n",
      "7 0 0.0005 0.00019451125990599394 0.9998112495281238\n",
      "7 200 0.0005 0.0001274886744795367 1.0\n",
      "7 400 0.0005 3.002821176778525e-05 1.0\n",
      "7 600 0.0005 3.464943074504845e-05 1.0\n",
      "7 800 0.0005 4.98495610372629e-05 1.0\n",
      "7 1000 0.0005 5.963986768620089e-05 1.0\n",
      "7 1200 0.0005 2.594349643914029e-05 1.0\n",
      "7 1400 0.0005 0.00015241125947795808 1.0\n",
      "7 1600 0.0005 2.965766361739952e-05 1.0\n",
      "7 1800 0.0005 1.4268438462750055e-05 1.0\n",
      "7 2000 0.0005 6.99391239322722e-05 1.0\n",
      "7 2200 0.0005 7.883836224209517e-05 1.0\n",
      "7 2400 0.0005 4.0281614928971976e-05 1.0\n",
      "7 2600 0.0005 0.00014390647993423045 1.0\n",
      "7 2800 0.0005 2.4875096642063e-05 1.0\n",
      "7 3000 0.0005 0.00685318885371089 0.9992432841468029\n",
      "7 3200 0.0005 3.0667968530906364e-05 1.0\n",
      "7 3400 0.0005 0.0004097237251698971 0.9996259584813915\n",
      "7 3600 0.0005 0.00030590692767873406 0.9998066885752948\n",
      "7 3800 0.0005 8.173292735591531e-05 1.0\n",
      "7 4000 0.0005 0.00012993217387702316 1.0\n",
      "7 4200 0.0005 0.00016033218707889318 1.0\n",
      "7 4400 0.0005 2.5954796001315117e-05 1.0\n",
      "7 4600 0.0005 3.396094325580634e-05 1.0\n",
      "7 4800 0.0005 2.1239531633909792e-05 1.0\n",
      "7 5000 0.0005 0.00028484134236350656 0.9998108210367007\n",
      "7 5200 0.0005 0.00025238897069357336 0.9998111782477341\n",
      "7 5400 0.0005 2.3388634872389957e-05 1.0\n",
      "7 5600 0.0005 3.594859299482778e-05 1.0\n",
      "7 5800 0.0005 8.319774497067556e-05 1.0\n",
      "7 6000 0.0005 0.00018940925656352192 0.9998069498069498\n",
      "7 6200 0.0005 0.00011440183880040422 1.0\n",
      "7 6400 0.0005 2.204162592533976e-05 1.0\n",
      "7 6600 0.0005 6.223394302651286e-05 1.0\n",
      "7 6800 0.0005 3.0537750717485324e-05 1.0\n",
      "7 7000 0.0005 1.3342672900762409e-05 1.0\n",
      "7 7200 0.0005 4.186329533695243e-06 1.0\n",
      "7 7400 0.0005 6.976821168791503e-06 1.0\n",
      "7 7600 0.0005 4.317753700888716e-05 1.0\n",
      "7 7800 0.0005 0.0026616721879690886 0.9996145692811718\n",
      "8 0 0.0005 0.00017363362712785602 1.0\n",
      "8 200 0.0005 8.791869913693517e-05 1.0\n",
      "8 400 0.0005 4.013832221971825e-05 1.0\n",
      "8 600 0.0005 1.8184589862357825e-05 1.0\n",
      "8 800 0.0005 0.0002497791138011962 1.0\n",
      "8 1000 0.0005 3.991752237197943e-05 1.0\n",
      "8 1200 0.0005 1.756663914420642e-05 1.0\n",
      "8 1400 0.0005 5.385641998145729e-05 1.0\n",
      "8 1600 0.0005 8.206056372728199e-05 1.0\n",
      "8 1800 0.0005 1.655072992434725e-05 1.0\n",
      "8 2000 0.0005 9.460575711273123e-06 1.0\n",
      "8 2200 0.0005 0.00013911142013967037 1.0\n",
      "8 2400 0.0005 0.0004283994494471699 0.9998088319632957\n",
      "8 2600 0.0005 1.5575384168187156e-05 1.0\n",
      "8 2800 0.0005 1.3369121006689966e-05 1.0\n",
      "8 3000 0.0005 0.00011290093243587762 1.0\n",
      "8 3200 0.0005 6.608104740735143e-05 1.0\n",
      "8 3400 0.0005 0.00011205077316844836 1.0\n",
      "8 3600 0.0005 2.00359063455835e-05 1.0\n",
      "8 3800 0.0005 2.1876934624742717e-05 1.0\n",
      "8 4000 0.0005 4.9935544666368514e-05 1.0\n",
      "8 4200 0.0005 2.886982292693574e-05 1.0\n",
      "8 4400 0.0005 6.140120967756957e-05 1.0\n",
      "8 4600 0.0005 3.719167216331698e-05 1.0\n",
      "8 4800 0.0005 0.00015465772594325244 1.0\n",
      "8 5000 0.0005 8.05067247711122e-06 1.0\n",
      "8 5200 0.0005 0.00018526488565839827 1.0\n",
      "8 5400 0.0005 0.0010634155478328466 0.9996214988644966\n",
      "8 5600 0.0005 3.8037887861719355e-05 1.0\n",
      "8 5800 0.0005 6.0329581174300984e-05 1.0\n",
      "8 6000 0.0005 1.5783201888552867e-05 1.0\n",
      "8 6200 0.0005 7.371239917119965e-05 1.0\n",
      "8 6400 0.0005 7.471418211935088e-05 1.0\n",
      "8 6600 0.0005 0.0005767008988186717 0.9998073588903872\n",
      "8 6800 0.0005 8.79552862897981e-06 1.0\n",
      "8 7000 0.0005 1.7055883290595375e-05 1.0\n",
      "8 7200 0.0005 2.407327338005416e-05 1.0\n",
      "8 7400 0.0005 2.8432818908186164e-06 1.0\n",
      "8 7600 0.0005 0.00011613752576522529 1.0\n",
      "8 7800 0.0005 7.423932856909232e-06 1.0\n",
      "9 0 0.00025 0.00028520182240754366 0.9998086124401914\n",
      "9 200 0.00025 3.2807317893457366e-06 1.0\n",
      "9 400 0.00025 1.502870782132959e-05 1.0\n",
      "9 600 0.00025 2.8872989787487313e-05 1.0\n",
      "9 800 0.00025 2.122836303897202e-05 1.0\n",
      "9 1000 0.00025 6.629521521972492e-05 1.0\n",
      "9 1200 0.00025 6.453533569583669e-05 1.0\n",
      "9 1400 0.00025 1.994125113924383e-06 1.0\n",
      "9 1600 0.00025 4.233612708048895e-05 1.0\n",
      "9 1800 0.00025 9.640603821026161e-06 1.0\n",
      "9 2000 0.00025 2.2230060494621284e-05 1.0\n",
      "9 2200 0.00025 4.254708255757578e-05 1.0\n",
      "9 2400 0.00025 0.0002993954985868186 0.9998097412480974\n",
      "9 2600 0.00025 2.4799198854452698e-06 1.0\n",
      "9 2800 0.00025 0.0005837357020936906 0.9998066511987626\n",
      "9 3000 0.00025 5.776004763902165e-05 1.0\n",
      "9 3200 0.00025 1.1470894605736248e-05 1.0\n",
      "9 3400 0.00025 1.4697718597744824e-06 1.0\n",
      "9 3600 0.00025 2.3197482732939534e-05 1.0\n",
      "9 3800 0.00025 5.147169213159941e-05 1.0\n",
      "9 4000 0.00025 2.3171322027337737e-05 1.0\n",
      "9 4200 0.00025 2.0266192223061807e-05 1.0\n",
      "9 4400 0.00025 1.8405595255899243e-05 1.0\n",
      "9 4600 0.00025 8.874670311342925e-05 1.0\n",
      "9 4800 0.00025 8.939106919569895e-05 1.0\n",
      "9 5000 0.00025 3.918463789887028e-06 1.0\n",
      "9 5200 0.00025 1.6433225482614944e-06 1.0\n",
      "9 5400 0.00025 3.732700497494079e-05 1.0\n",
      "9 5600 0.00025 3.414577349758474e-06 1.0\n",
      "9 5800 0.00025 6.216023393790238e-06 1.0\n",
      "9 6000 0.00025 0.00014957327221054584 1.0\n",
      "9 6200 0.00025 2.72320303338347e-05 1.0\n",
      "9 6400 0.00025 3.329168612253852e-05 1.0\n",
      "9 6600 0.00025 1.0728369488788303e-05 1.0\n",
      "9 6800 0.00025 2.869344643841032e-05 1.0\n",
      "9 7000 0.00025 6.211006257217377e-05 1.0\n",
      "9 7200 0.00025 9.50104458752321e-06 1.0\n",
      "9 7400 0.00025 3.1101666536414996e-05 1.0\n",
      "9 7600 0.00025 6.256753295019735e-06 1.0\n",
      "9 7800 0.00025 8.585892828705255e-06 1.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for i,(x,y) in enumerate(loader):\n",
    "        # x->[b,50]\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        \n",
    "        # 在训练时,是拿y的每一个字符输入,预测下一个字符,所以不需要最后一个字\n",
    "        pred = model(x, y[:, :-1])\n",
    "        # pred->[b,50,39]->[b*50,39]\n",
    "        pred = pred.reshape(-1, 39)\n",
    "        # y:[b, 51] -> [b*50]\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "\n",
    "        # 忽略pad的数据\n",
    "        select = y != zidian_y['<PAD>']\n",
    "        pred = pred[select]\n",
    "        y = y[select]\n",
    "\n",
    "        loss = loss_func(pred, y)\n",
    "        current_batch_loss = loss.item()\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # [select, 39] -> [select]\n",
    "        if i%200==0:\n",
    "            pred = pred.argmax(1)\n",
    "            correct = (pred == y).sum().item()\n",
    "            accuracy = correct / len(pred)\n",
    "            lr = optim.param_groups[0]['lr']\n",
    "            print(epoch, i, lr, loss.item(), accuracy)\n",
    "\n",
    "    sched.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5566d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c9d2755",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_scrach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
