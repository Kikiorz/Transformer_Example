{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "686ce9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import einops\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d70e1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载器定义完成\n"
     ]
    }
   ],
   "source": [
    "# 数据生成部分，定义词典（数据生成的范围）\n",
    "# 定义字典\n",
    "zidian_x = '<SOS>,<EOS>,<PAD>,0,1,2,3,4,5,6,7,8,9,q,w,e,r,t,y,u,i,o,p,a,s,d,f,g,h,j,k,l,z,x,c,v,b,n,m'\n",
    "zidian_x = {word: i for i, word in enumerate(zidian_x.split(','))}\n",
    "\n",
    "zidian_xr = [k for k, v in zidian_x.items()]\n",
    "\n",
    "zidian_y = {k.upper(): v for k, v in zidian_x.items()}\n",
    "\n",
    "zidian_yr = [k for k, v in zidian_y.items()]\n",
    "# 生成数据逻辑就是：X是正序小写数字，Y是逆序大写互补数字\n",
    "def get_data():\n",
    "    # 定义词集合\n",
    "    words = [\n",
    "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'q', 'w', 'e', 'r',\n",
    "        't', 'y', 'u', 'i', 'o', 'p', 'a', 's', 'd', 'f', 'g', 'h', 'j', 'k',\n",
    "        'l', 'z', 'x', 'c', 'v', 'b', 'n', 'm'\n",
    "    ]\n",
    "\n",
    "    # 定义每个词被选中的概率\n",
    "    p = np.array([\n",
    "        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
    "        13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26\n",
    "    ])\n",
    "    p = p / p.sum()\n",
    "\n",
    "    # 随机选n个词\n",
    "    n = random.randint(30, 48)\n",
    "    x = np.random.choice(words, size=n, replace=True, p=p)\n",
    "\n",
    "    # 采样的结果就是x\n",
    "    x = x.tolist()\n",
    "\n",
    "    # y是对x的变换得到的\n",
    "    # 字母大写,数字取10以内的互补数\n",
    "    def f(i):\n",
    "        i = i.upper()\n",
    "        if not i.isdigit():\n",
    "            return i\n",
    "        i = 9 - int(i)\n",
    "        return str(i)\n",
    "\n",
    "    y = [f(i) for i in x]\n",
    "    y = y + [y[-1]]\n",
    "    # 逆序\n",
    "    y = y[::-1]\n",
    "\n",
    "    # 加上首尾符号\n",
    "    x = ['<SOS>'] + x + ['<EOS>']\n",
    "    y = ['<SOS>'] + y + ['<EOS>']\n",
    "\n",
    "    # 补pad到固定长度\n",
    "    x = x + ['<PAD>'] * 50\n",
    "    y = y + ['<PAD>'] * 51\n",
    "    x = x[:50]\n",
    "    y = y[:51]\n",
    "\n",
    "    # 编码成数据\n",
    "    x = [zidian_x[i] for i in x]\n",
    "    y = [zidian_y[i] for i in y]\n",
    "\n",
    "    # 转tensor\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# 定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1000000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return get_data()\n",
    "\n",
    "# 数据加载器\n",
    "\n",
    "print('数据加载器定义完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f23c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask\n",
    "import torch\n",
    "\n",
    "from data import zidian_x, zidian_y\n",
    "\n",
    "# mask pad\n",
    "def mask_pad(data):\n",
    "    mask = data == zidian_x['<PAD>']\n",
    "\n",
    "    # [b, 50] -> [b, 1, 1, 50]\n",
    "    mask = mask.reshape(-1, 1, 1, 50)\n",
    "\n",
    "    # 在计算注意力时,是计算50个词和50个词相互之间的注意力,所以是个50*50的矩阵\n",
    "    # 是pad的列是true,意味着任何词对pad的注意力都是0\n",
    "    # 但是pad本身对其他词的注意力并不是0\n",
    "    # 所以是pad的行不是true\n",
    "\n",
    "    # 复制n次\n",
    "    # [b, 1, 1, 50] -> [b, 1, 50, 50]\n",
    "    mask = mask.expand(-1, 1, 50, 50)\n",
    "\n",
    "    return mask\n",
    "\n",
    "# 在Y预测阶段变为上三角\n",
    "def mask_tril(data):\n",
    "    # b句话,每句话50个词,这里是还没embed的\n",
    "    # data = [b, 50]\n",
    "\n",
    "    # 50*50的矩阵表示每个词对其他词是否可见\n",
    "    # 上三角矩阵,不包括对角线,意味着,对每个词而言,他只能看到他自己,和他之前的词,而看不到之后的词\n",
    "    # [1, 50, 50]\n",
    "    \"\"\"\n",
    "    [[0, 1, 1, 1, 1],\n",
    "     [0, 0, 1, 1, 1],\n",
    "     [0, 0, 0, 1, 1],\n",
    "     [0, 0, 0, 0, 1],\n",
    "     [0, 0, 0, 0, 0]]\"\"\"\n",
    "    tril = 1 - torch.tril(torch.ones(1, 50, 50, dtype=torch.long)).to(device)\n",
    "\n",
    "    # 判断y当中每个词是不是pad,如果是pad则不可见\n",
    "    # [b, 50]\n",
    "    mask = data == zidian_y['<PAD>']\n",
    "\n",
    "    # 变形+转型,为了之后的计算\n",
    "    # [b, 1, 50]\n",
    "    mask = mask.unsqueeze(1).long()\n",
    "\n",
    "    # mask和tril求并集\n",
    "    # [b, 1, 50] + [1, 50, 50] -> [b, 50, 50]\n",
    "    mask = mask + tril\n",
    "\n",
    "    # 转布尔型\n",
    "    mask = mask > 0\n",
    "\n",
    "    # 转布尔型,增加一个维度,便于后续的计算\n",
    "    mask = (mask == 1).unsqueeze(dim=1)\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8b671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myPositionEmbedding, 位置编码\n",
    "class myPositionEmbedding(torch.nn.Module):\n",
    "    def __init__(self,voc_size,max_len,emb_dim):\n",
    "        super().__init__()\n",
    "        # 创建嵌入层 \"b,50->b,50,32\"\n",
    "        self.emb=torch.nn.Embedding(voc_size,emb_dim)\n",
    "        self.emb.weight.data.normal_(0, 0.1)\n",
    "\n",
    "        # 创建PE参数记得把他移动到模型中哦\n",
    "        pe=torch.zeros(max_len, emb_dim,dtype=torch.float32)\n",
    "        # for嵌套复杂度太高了，\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        i = torch.arange(0, emb_dim, 2, dtype=torch.float32)\n",
    "        inv_freq = 1.0 / (1.0e-3 ** (i / emb_dim))\n",
    "        # 广播机制（50,1） （32）\n",
    "        argument = pos * inv_freq\n",
    "        pe[:, 0::2] = torch.sin(argument)\n",
    "        pe[:, 1::2] = torch.cos(argument)\n",
    "        # 送到模型中，并且不更新\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.emb(x)\n",
    "        return x+self.pe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86d4564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头注意力机制\n",
    "class MultiHead(torch.nn.Module):\n",
    "    def __init__(self,head_num,in_ch):\n",
    "        super().__init__()\n",
    "        self.wq=torch.nn.Linear(in_ch,in_ch)\n",
    "        self.wk=torch.nn.Linear(in_ch,in_ch)\n",
    "        self.wv=torch.nn.Linear(in_ch,in_ch)\n",
    "        self.num_head=head_num\n",
    "        self.in_ch=in_ch\n",
    "        self.norm = torch.nn.LayerNorm(normalized_shape=in_ch, elementwise_affine=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.out_fc=torch.nn.Linear(in_ch,in_ch)\n",
    "\n",
    "    def forward(self,x,mask,cros_flag,VX=0):\n",
    "        # x->[b,l,d]\n",
    "        if cros_flag==0:\n",
    "            res=x.clone()\n",
    "            x = self.norm(x)\n",
    "            Q= einops.rearrange(self.wq(x),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "            K=einops.rearrange(self.wk(x),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "            V=einops.rearrange(self.wv(x),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "\n",
    "            z=torch.softmax(((Q@K.permute(0, 1, 3, 2))/((self.in_ch/self.num_head)**0.5)).masked_fill_(mask, -float('inf')),dim=-1)@V\n",
    "\n",
    "            z=einops.rearrange(z,\"... a b c -> ... b (a c)\")\n",
    "\n",
    "            return self.dropout(self.out_fc(z))+res\n",
    "        else:\n",
    "            res=x.clone()\n",
    "            x = self.norm(x)\n",
    "            VX= self.norm(VX)\n",
    "            Q= einops.rearrange(self.wq(x),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "            K=einops.rearrange(self.wk(VX),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "            V=einops.rearrange(self.wv(VX),\"... b (d e) ->... d b e \",d=self.num_head)\n",
    "\n",
    "            z=torch.softmax(((Q@K.permute(0, 1, 3, 2))/((self.in_ch/self.num_head)**0.5)).masked_fill_(mask, -float('inf')),dim=-1)@V\n",
    "\n",
    "            z=einops.rearrange(z,\"... a b c -> ... b (a c)\")\n",
    "\n",
    "            return self.dropout(self.out_fc(z))+res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7ca4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全连接组成\n",
    "class FullyConnected(torch.nn.Module):\n",
    "    def __init__(self,in_ch):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=in_ch, out_features=in_ch*2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=in_ch*2, out_features=in_ch),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "        )\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(normalized_shape=32,\n",
    "        elementwise_affine=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        res=x.clone()\n",
    "        x=self.norm(x)\n",
    "        x=self.fc(x)\n",
    "        return x+res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d92aeb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器组成\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self,num_head,in_ch ):\n",
    "        super().__init__()\n",
    "        self.mul=MultiHead(num_head,in_ch=in_ch)\n",
    "        self.fc=FullyConnected(in_ch)\n",
    "    def forward(self,x,mask):\n",
    "        # x [b,long,emb]\n",
    "        # 多头注意力\n",
    "        x=self.mul(x,mask,cros_flag=0)\n",
    "        # 前馈神经网络\n",
    "        # x [b,long,emb]\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,num_head,in_ch):\n",
    "        super().__init__()\n",
    "        self.layer_1 = EncoderLayer(num_head,in_ch)\n",
    "        self.layer_2 = EncoderLayer(num_head,in_ch)\n",
    "        self.layer_3 = EncoderLayer(num_head,in_ch)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.layer_1(x, mask)\n",
    "        x = self.layer_2(x, mask)\n",
    "        x = self.layer_3(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e8d5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器层\n",
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self,num_head,in_ch):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mul1 = MultiHead(num_head,in_ch)\n",
    "        self.mul2 = MultiHead(num_head,in_ch)\n",
    "\n",
    "        self.fc = FullyConnected(in_ch)\n",
    "\n",
    "    def forward(self, x, y, mask_pad_x, mask_tril_y):\n",
    "        # 先计算y的自注意力,维度不变\n",
    "        y = self.mul1(y, mask_tril_y, cros_flag=0)\n",
    "        # 结合x和y的注意力计算,维度不变\n",
    "        # [b, 50, 32],[b, 50, 32] -> [b, 50, 32]\n",
    "        y = self.mul2(y, mask_pad_x, cros_flag=1,VX=x)\n",
    "        y = self.fc(y)\n",
    "        return y\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self,num_head,in_ch):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_1 = DecoderLayer(num_head,in_ch)\n",
    "        self.layer_2 = DecoderLayer(num_head,in_ch)\n",
    "        self.layer_3 = DecoderLayer(num_head,in_ch)\n",
    "\n",
    "    def forward(self, x, y, mask_pad_x, mask_tril_y):\n",
    "        y = self.layer_1(x, y, mask_pad_x, mask_tril_y)\n",
    "        y = self.layer_2(x, y, mask_pad_x, mask_tril_y)\n",
    "        y = self.layer_3(x, y, mask_pad_x, mask_tril_y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28396e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self,voc_size,max_len,num_head,in_ch):\n",
    "        super().__init__()\n",
    "        self.embed_x = myPositionEmbedding(voc_size,max_len,in_ch)\n",
    "        self.embed_y = myPositionEmbedding(voc_size,max_len,in_ch)\n",
    "        self.encoder = Encoder(num_head,in_ch)\n",
    "        self.decoder = Decoder(num_head,in_ch)\n",
    "        self.fc_out = torch.nn.Linear(32, 39)\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        mask_x=mask_pad(x)\n",
    "        mask_y=mask_tril(y)\n",
    "        x=self.embed_x(x)\n",
    "        y=self.embed_y(y)\n",
    "        # x[b,50]->[b,50,32]\n",
    "        x=self.encoder(x,mask_x)\n",
    "        y=self.decoder(x,y,mask_x,mask_y)\n",
    "        y=self.fc_out(y)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "592c13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset=Dataset(),\n",
    "                                     batch_size=32,\n",
    "                                     drop_last=True,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=None)\n",
    "model = Transformer(39,50,4,32)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "loss_func.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "sched = torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef9bb327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.002 3.882072925567627 0.023455824863174355\n",
      "0 200 2.710505431213761e-23 3.53379225730896 0.04293116210214656\n",
      "0 400 1.8367099231598243e-43 3.537034749984741 0.05361131794489948\n",
      "0 600 1.2446030555722284e-63 3.569934368133545 0.04849884526558892\n",
      "0 800 1.6867516709168838e-83 3.5291905403137207 0.05100463678516229\n",
      "0 1000 1.142987391282275e-103 3.5222105979919434 0.06651549508692366\n",
      "0 1200 7.745183829698637e-124 3.5566647052764893 0.05615384615384615\n",
      "0 1400 1.0496681418073576e-143 3.5308995246887207 0.057462686567164176\n",
      "0 1600 7.112827998352248e-164 3.515723943710327 0.05417276720351391\n",
      "0 1800 4.819839730205768e-184 3.528536081314087 0.05315110098709187\n",
      "0 2000 6.532100883151302e-204 3.542149066925049 0.05332302936630603\n",
      "0 2200 4.426323730254452e-224 3.544767141342163 0.04836309523809524\n",
      "0 2400 2.999393627791262e-244 3.56874418258667 0.04357976653696498\n",
      "0 2600 4.064936359238081e-264 3.5968692302703857 0.05084745762711865\n",
      "0 2800 2.7545080198132776e-284 3.5513663291931152 0.053231939163498096\n",
      "0 3000 1.8665272370064378e-304 3.552964448928833 0.050381679389312976\n",
      "0 3200 0.0 3.541769504547119 0.04794007490636704\n",
      "0 3400 0.0 3.5430908203125 0.058383233532934134\n",
      "0 3600 0.0 3.5725128650665283 0.050190114068441066\n",
      "0 3800 0.0 3.5316824913024902 0.059052059052059055\n",
      "0 4000 0.0 3.5634241104125977 0.05627376425855513\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m current_batch_loss = loss.item()\n\u001b[32m     21\u001b[39m optim.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m optim.step()\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# [select, 39] -> [select]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_scrach/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_scrach/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_scrach/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for i,(x,y) in enumerate(loader):\n",
    "        # x->[b,50]\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        \n",
    "        # 在训练时,是拿y的每一个字符输入,预测下一个字符,所以不需要最后一个字\n",
    "        pred = model(x, y[:, :-1])\n",
    "        # pred->[b,50,39]->[b*50,39]\n",
    "        pred = pred.reshape(-1, 39)\n",
    "        # y:[b, 51] -> [b*50]\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "\n",
    "        # 忽略pad的数据\n",
    "        select = y != zidian_y['<PAD>']\n",
    "        pred = pred[select]\n",
    "        y = y[select]\n",
    "\n",
    "        loss = loss_func(pred, y)\n",
    "        current_batch_loss = loss.item()\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # [select, 39] -> [select]\n",
    "        if i%200==0:\n",
    "            pred = pred.argmax(1)\n",
    "            correct = (pred == y).sum().item()\n",
    "            accuracy = correct / len(pred)\n",
    "            lr = optim.param_groups[0]['lr']\n",
    "            print(epoch, i, lr, loss.item(), accuracy)\n",
    "\n",
    "        sched.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5566d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c9d2755",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_scrach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
